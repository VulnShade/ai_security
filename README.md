# AI Security Notes 🛡️

A collection of personal notes on AI security, focusing on LLM security, penetration testing, red teaming techniques, defensive measures, and secure configurations.

## 📚 Table of Contents

### 🎯 Red Team
- [LLM Vulnerabilities](red_teaming/vulnerabilities.md) - Common vulnerabilities in LLM applications
- [Attack Techniques](red_teaming/redteam_techniques.md) - Methods for testing and bypassing LLM safeguards
- [Prompt Injection Automation](red_teaming/tools/prompt_injection/automation.md) - Automated testing of prompt injection attacks

### 🛡️ Blue Team
*(Coming soon)*

### 🔧 Secure Configuration
*(Coming soon)*

## 🎯 Purpose

This repository serves as a knowledge base for:
- Understanding LLM security risks and vulnerabilities
- Exploring red teaming techniques for AI systems
- Implementing defensive measures
- Documenting secure configuration practices

The focus is primarily on practical approaches to AI security, with real-world examples and techniques that can be applied to improve the security posture of AI systems.

## 🔗 External Resources

### Official Documentation & Databases
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/)
- [AI Incident Database](https://incidentdatabase.ai/apps/incidents/)
- [AI Vulnerability Database](https://avidml.org/database/)

---
**Note**: These notes are maintained for educational purposes and should be used responsibly and ethically.
