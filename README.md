# AI Security Notes ğŸ›¡ï¸

A collection of personal notes on AI security, focusing on LLM security, penetration testing, red teaming techniques, defensive measures, and secure configurations.

## ğŸ“š Table of Contents

### ğŸ¯ Red Team
- [LLM Vulnerabilities](red_teaming/vulnerabilities.md) - Common vulnerabilities in LLM applications
- [Attack Techniques](red_teaming/redteam_techniques.md) - Methods for testing and bypassing LLM safeguards
- [Prompt Injection Automation](red_teaming/tools/prompt_injection/automation.md) - Automated testing of prompt injection attacks

### ğŸ›¡ï¸ Blue Team
*(Coming soon)*

### ğŸ”§ Secure Configuration
*(Coming soon)*

## ğŸ¯ Purpose

This repository serves as a knowledge base for:
- Understanding LLM security risks and vulnerabilities
- Exploring red teaming techniques for AI systems
- Implementing defensive measures
- Documenting secure configuration practices

The focus is primarily on practical approaches to AI security, with real-world examples and techniques that can be applied to improve the security posture of AI systems.

## ğŸ”— External Resources

### Official Documentation & Databases
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/)
- [AI Incident Database](https://incidentdatabase.ai/apps/incidents/)
- [AI Vulnerability Database](https://avidml.org/database/)

---
**Note**: These notes are maintained for educational purposes and should be used responsibly and ethically.
